{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "JTj03wGlnnD9",
   "metadata": {
    "id": "JTj03wGlnnD9"
   },
   "source": [
    "# 1. Collecting data set and Importing necessary libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VLZ8t-vCn7VP",
   "metadata": {
    "id": "VLZ8t-vCn7VP"
   },
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b6805a",
   "metadata": {
    "id": "76b6805a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rXn4RjIiNikm",
   "metadata": {
    "id": "rXn4RjIiNikm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WriA7y8Sn_xO",
   "metadata": {
    "id": "WriA7y8Sn_xO"
   },
   "source": [
    "#### 1.2 Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba50864",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bba50864",
    "outputId": "6ffadfd7-a8bb-47d8-b3b0-b4bc33640d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bff0e2",
   "metadata": {
    "id": "52bff0e2"
   },
   "outputs": [],
   "source": [
    "text_folder = '/content/drive/My Drive/shakespeares-works_TXT_FolgerShakespeare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603ecda",
   "metadata": {
    "id": "2603ecda"
   },
   "outputs": [],
   "source": [
    "#Reading input text files\n",
    "text = []\n",
    "names = []\n",
    "for root, dir, files in os.walk(text_folder):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), 'r') as rd:\n",
    "            text.append(rd.read())\n",
    "            names.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02Pj8YTNoE2S",
   "metadata": {
    "id": "02Pj8YTNoE2S"
   },
   "source": [
    "# 2. Removal of punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcb297",
   "metadata": {
    "id": "76fcb297"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# tokenise the document\n",
    "def tokenize(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    token_words= [word for word in words if word.isalnum()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b284948",
   "metadata": {
    "id": "8b284948"
   },
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopwords = stopwords.words('english')\n",
    "def stopwords_clr(sentence):\n",
    "    tokens_clr = [token for token in sentence if token not in stopwords]\n",
    "    return tokens_clr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4v98f8ioUht",
   "metadata": {
    "id": "p4v98f8ioUht"
   },
   "source": [
    "# 3. Normalization using Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bb44b",
   "metadata": {
    "id": "051bb44b"
   },
   "outputs": [],
   "source": [
    "#Stemming the words to root form\n",
    "stem = PorterStemmer()\n",
    "\n",
    "def stem_tokens(sentence):\n",
    "    tokens_stem = []\n",
    "    for token in sentence:\n",
    "        tokens_stem.append(stem.stem(token))\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DYnu1zvHoq-n",
   "metadata": {
    "id": "DYnu1zvHoq-n"
   },
   "source": [
    "# 4. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-V_jGbRWHGqh",
   "metadata": {
    "id": "-V_jGbRWHGqh"
   },
   "outputs": [],
   "source": [
    "def preprocess(cont):\n",
    "    return \" \".join(stopwords_clr(stem_tokens(stopwords_clr(tokenize(cont)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L553rLkxHGqh",
   "metadata": {
    "id": "L553rLkxHGqh"
   },
   "outputs": [],
   "source": [
    "processed_data = []    #This contains the pre-processed data of each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BEaCdwWYHGqh",
   "metadata": {
    "id": "BEaCdwWYHGqh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "  processed_data.append(preprocess(text[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feba98c",
   "metadata": {
    "id": "3feba98c"
   },
   "source": [
    "# 5. Construct inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5a308",
   "metadata": {
    "id": "d6e5a308"
   },
   "outputs": [],
   "source": [
    "inv_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60460f",
   "metadata": {
    "id": "0d60460f"
   },
   "outputs": [],
   "source": [
    "#Indexing the inputted document\n",
    "def indexing(document, index):\n",
    "    words = nltk.word_tokenize(document)\n",
    "    for word in words:\n",
    "        if(inv_index.get(word) is None):\n",
    "            inv_index[word] = [index]\n",
    "        elif not index in inv_index.get(word):\n",
    "            inv_index.get(word).append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9be9a",
   "metadata": {
    "id": "c9d9be9a"
   },
   "outputs": [],
   "source": [
    "for x in range(len(processed_data)):\n",
    "    indexing(processed_data[x], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SNjSGfnEOsXN",
   "metadata": {
    "id": "SNjSGfnEOsXN"
   },
   "outputs": [],
   "source": [
    "keys = list(inv_index.keys())      #Keys contains a list of all terms in the dictionary\n",
    "postings = list(inv_index.values())    #Postings contain the posting list of all terms in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "su_LuKmoVKlB",
   "metadata": {
    "id": "su_LuKmoVKlB"
   },
   "source": [
    "# 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A8H_59jAR8uu",
   "metadata": {
    "id": "A8H_59jAR8uu"
   },
   "outputs": [],
   "source": [
    "#To find Levenshtein distance of two terms\n",
    "def levenshtein_distance(term1, term2):\n",
    "    term1 = term1.lower()\n",
    "    term2 = term2.lower()\n",
    "    dyn_mat = [[0 for x in range(len(term2) + 1)] for x in range(len(term1) + 1)]\n",
    "\n",
    "    for x in range(len(term1) + 1):\n",
    "        dyn_mat[x][0] = x\n",
    "    for y in range(len(term2) + 1):\n",
    "        dyn_mat[0][y] = y\n",
    "\n",
    "    for x in range(1, len(term1) + 1):\n",
    "        for y in range(1, len(term2) + 1):\n",
    "            if term1[x - 1] == term2[y - 1]:\n",
    "                dyn_mat[x][y] = min(\n",
    "                    dyn_mat[x - 1][y] + 1,\n",
    "                    dyn_mat[x - 1][y - 1],\n",
    "                    dyn_mat[x][y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                dyn_mat[x][y] = min(\n",
    "                    dyn_mat[x - 1][y] + 1,\n",
    "                    dyn_mat[x - 1][y - 1] + 1,\n",
    "                    dyn_mat[x][y - 1] + 1\n",
    "                )\n",
    "\n",
    "    return dyn_mat[len(term1)][len(term2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3P5nizrnTDj-",
   "metadata": {
    "id": "3P5nizrnTDj-"
   },
   "outputs": [],
   "source": [
    "#To find the nearest word in the dictionary to a misspelled word\n",
    "def nearest_word(word):\n",
    "  min = 100\n",
    "  near = ''\n",
    "  for key in keys:\n",
    "    leven = levenshtein_distance(word, key)\n",
    "    if leven == 0:\n",
    "      min = leven\n",
    "      near = key\n",
    "      return near\n",
    "    if leven < min:\n",
    "      near = key\n",
    "      min = leven\n",
    "  return near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JzpVXkOsotpq",
   "metadata": {
    "id": "JzpVXkOsotpq"
   },
   "outputs": [],
   "source": [
    "def gen_posting(term, inv_index):\n",
    "  near = nearest_word(term)\n",
    "  posting = inv_index[near]\n",
    "  return posting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZWDuvwWloAa_",
   "metadata": {
    "id": "ZWDuvwWloAa_"
   },
   "source": [
    "# 7. Wildcard Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8q5YKLxGB",
   "metadata": {
    "id": "a4b8q5YKLxGB"
   },
   "outputs": [],
   "source": [
    "def rotate(s, n):\n",
    "    return s[n:] + s[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xLBPiqQQOZUj",
   "metadata": {
    "id": "xLBPiqQQOZUj"
   },
   "outputs": [],
   "source": [
    "def bit_and(X, Y):\n",
    "    return set(X).intersection(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BOgAxcsEfed1",
   "metadata": {
    "id": "BOgAxcsEfed1"
   },
   "outputs": [],
   "source": [
    "def bit_or(X, Y):\n",
    "    return set(X).union(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lydl1LXWPY5-",
   "metadata": {
    "id": "lydl1LXWPY5-"
   },
   "outputs": [],
   "source": [
    "#Generating all permuterms for a term\n",
    "def gen_perm(keys, per_index):\n",
    "  for key in keys:\n",
    "    okey = key + \"$\"\n",
    "    for i in range(len(okey),0,-1):\n",
    "      rot = rotate(okey, i)\n",
    "      per_index[rot] = key\n",
    "  return per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C6URF2nOL2dv",
   "metadata": {
    "id": "C6URF2nOL2dv"
   },
   "outputs": [],
   "source": [
    "#Find all appropriate permuterms and original terms\n",
    "def find_perm(term, prefix):\n",
    "    req_terms = []\n",
    "    for key in term.keys():\n",
    "        if key.startswith(prefix):\n",
    "            req_terms.append(term[key])\n",
    "    return req_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LTHKrNvyMhBh",
   "metadata": {
    "id": "LTHKrNvyMhBh"
   },
   "outputs": [],
   "source": [
    "#For cases 1, 2 and 3\n",
    "def processQuery1(query, per_index):    \n",
    "    req_terms = find_perm(per_index, query)\n",
    "    print(req_terms)\n",
    "\n",
    "    post_ID = []\n",
    "    for term in req_terms:\n",
    "        post_ID.append(inv_index[term])\n",
    "    print(post_ID)\n",
    "\n",
    "    coll = []\n",
    "    for x in post_ID:\n",
    "        for y in x:\n",
    "            coll.append(y)   \n",
    "\n",
    "    coll = [int(x) for x in coll]\n",
    "    per = set(coll)\n",
    "    per = list(per) \n",
    "    print(per)    \n",
    "\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1UHqDSZwNBEU",
   "metadata": {
    "id": "1UHqDSZwNBEU"
   },
   "outputs": [],
   "source": [
    "#For case 4 (X*Y*Z)\n",
    "def processQuery2(que_part1, que_part2, per_index):\n",
    "\n",
    "  #Part 1 = Z$X\n",
    "  req_terms1 = find_perm(per_index, que_part1)\n",
    "  print(req_terms1)\n",
    "\n",
    "  post_ID1 = []\n",
    "  for term in req_terms1:\n",
    "      post_ID1.append(inv_index[term])\n",
    "  print(post_ID1)\n",
    "\n",
    "  coll1 = []\n",
    "  for x in post_ID1:\n",
    "      for y in x:\n",
    "          coll1.append(y) \n",
    "  print(coll1)  \n",
    "\n",
    "  #Part 2 = Y\n",
    "  req_terms2 = find_perm(per_index, que_part2)\n",
    "  print(req_terms2)\n",
    "\n",
    "  post_ID2 = []\n",
    "  for term in req_terms2:\n",
    "      post_ID2.append(inv_index[term])\n",
    "  print(post_ID2)\n",
    "\n",
    "  coll2 = []\n",
    "  for x in post_ID2:\n",
    "      for y in x:\n",
    "          coll2.append(y) \n",
    "  print(coll2)  \n",
    "\n",
    "  #Intersecting the two posting lists obtained above\n",
    "  '''req_terms_final = []\n",
    "  for x in req_terms1:\n",
    "    if x in req_terms2:\n",
    "      req_terms_final.append(x) \n",
    "  print(req_terms_final)''' \n",
    "\n",
    "  coll1 = [int(x) for x in coll1]\n",
    "  coll2 = [int(x) for x in coll2]\n",
    "\n",
    "  coll_final = bit_and(coll1, coll2)\n",
    "  per_final = set(coll_final)\n",
    "  per_final = list(per_final)\n",
    "  print(per_final)\n",
    "\n",
    "  return per_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p-1BHY1XLOcE",
   "metadata": {
    "id": "p-1BHY1XLOcE"
   },
   "outputs": [],
   "source": [
    "#Decide case and process the wildcard query accordingly\n",
    "def wildcard_process(query):\n",
    "  out = []\n",
    "  per_index = []\n",
    "  comps = query.split('*')\n",
    "  case = 0\n",
    "\n",
    "  if len(comps) == 3:\n",
    "    case = 4\n",
    "  elif comps[1] == '':\n",
    "    case = 1\n",
    "  elif comps[0] == '':\n",
    "    case = 2\n",
    "  elif comps[0] != '' and comps[1] != '':\n",
    "    case = 3\n",
    "\n",
    "  per_index = {}\n",
    "  per_index = gen_perm(keys, per_index)\n",
    "\n",
    "  if case == 1:\n",
    "    query = \"$\" + comps[0]\n",
    "  elif case == 2:\n",
    "    query = comps[1] + \"$\"\n",
    "  elif case == 3:\n",
    "    query = comps[1] + \"$\" + comps[0]\n",
    "  elif case == 4:\n",
    "    que_part1 = comps[2] + \"$\" + comps[0]\n",
    "    que_part2 = comps[1]\n",
    "    print(que_part1, que_part2)\n",
    "\n",
    "  if case != 4:\n",
    "    out = processQuery1(query, per_index)\n",
    "  elif case == 4:\n",
    "    out = processQuery2(que_part1, que_part2, per_index)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Qvtu23B2Eul",
   "metadata": {
    "id": "5Qvtu23B2Eul"
   },
   "source": [
    "# 6. Boolean query multiple terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pDwBCqAH2FUy",
   "metadata": {
    "id": "pDwBCqAH2FUy"
   },
   "outputs": [],
   "source": [
    "#Processing a boolean query and finding the appropriate documents\n",
    "def boolean_query(query, inv_index):\n",
    "  terms = query.split(' ')\n",
    "  bool_words = []\n",
    "  diff_words = []\n",
    "\n",
    "  for term in terms:\n",
    "    if term.lower() != 'and' and term.lower() != 'or' and term.lower() != 'not':\n",
    "      diff_words.append(term)\n",
    "    else:\n",
    "      bool_words.append(term)\n",
    "  \n",
    "  print(bool_words, diff_words)\n",
    "  \n",
    "  posting_term = []\n",
    "  posting_comb = []\n",
    "\n",
    "  for term in diff_words:\n",
    "    if '*' in term:\n",
    "      posting_term = wildcard_process(term)\n",
    "      posting_comb.append(posting_term)\n",
    "    else:\n",
    "      posting_term = gen_posting(term, inv_index)\n",
    "      posting_comb.append(posting_term)\n",
    "\n",
    "  print(posting_comb)\n",
    "\n",
    "\n",
    "  i = 0\n",
    "\n",
    "  while i < len(bool_words):\n",
    "\n",
    "    if bool_words[i] == 'not':\n",
    "      all_docs = set(list(range(len(processed_data))))\n",
    "      res = list(all_docs - set(posting_comb[0]))\n",
    "      posting_comb.remove(posting_comb[0])\n",
    "      posting_comb.insert(0, res)\n",
    "      i = i + 1\n",
    "\n",
    "    elif bool_words[i] == 'and':\n",
    "      if (i + 1) < len(bool_words) and bool_words[i + 1] == 'not':\n",
    "        all_docs = set(list(range(len(processed_data))))\n",
    "        res = list(all_docs - set(posting_comb[1]))\n",
    "        i = i + 1\n",
    "      else:\n",
    "        res = posting_comb[1]\n",
    "      intersection = list(set(posting_comb[0]).intersection(res))\n",
    "      posting_comb.remove(posting_comb[0])\n",
    "      posting_comb.remove(posting_comb[0])\n",
    "      posting_comb.insert(0, intersection)\n",
    "      i = i + 1\n",
    "\n",
    "    elif bool_words[i] == 'or':\n",
    "      if (i + 1) < len(bool_words) and bool_words[i + 1] == 'not':\n",
    "        all_docs = set(list(range(len(processed_data))))\n",
    "        res = list(all_docs - set(posting_comb[1]))\n",
    "        i = i + 1\n",
    "      else:\n",
    "        res = posting_comb[1]\n",
    "      union = posting_comb[0] + list(set(posting_comb[1]) - set(posting_comb[0]))\n",
    "      posting_comb.remove(posting_comb[0])\n",
    "      posting_comb.remove(posting_comb[0])\n",
    "      posting_comb.insert(0, union)\n",
    "      i = i + 1\n",
    "      \n",
    "  print(posting_comb)\n",
    "  return posting_comb[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WYIgVBpI9Lb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYIgVBpI9Lb6",
    "outputId": "7830cfa5-ab30-4678-cf1e-286d0aea8094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'not', 'or'] ['antony', 'clopara', 'calphurnia']\n",
      "[[17, 26, 39, 40], [4, 11, 18, 40], [17]]\n",
      "[[17, 26, 39]]\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "out = boolean_query('antony and not clopara or calphurnia', inv_index)\n",
    "output = open(\"OUTPUT_Documents.txt\",\"w\")\n",
    "\n",
    "for x in out:\n",
    "  output.write(names[x] + '\\n')\n",
    "\n",
    "output.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR_Assignment_1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
